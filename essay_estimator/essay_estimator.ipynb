{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "focal-festival",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center\">\n",
    "    <h3>Essay Score Estimator</h3>\n",
    "</div>\n",
    "<p style=\"text-align: center\">\n",
    "    How Jia Jean<br>\n",
    "    <i>30th April 2021</i>\n",
    "</p>   __________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facial-camel",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catholic-motivation",
   "metadata": {},
   "source": [
    "The objective of this study is to perform predictive analysis throught supervised machine learning on Essay-Features.csv dataset. The dataset contains the students' essay scoring and features that might have correspondent effect on the essay scoring such as characters, words, punctuations, average word length, synonym words, and others. The dataset undergone the process below:\n",
    "\n",
    "1. Import Libraries\n",
    "2. Importing, description, and modification of Data\n",
    "3. Supervised Learning\n",
    "    - Normalisation\n",
    "    - Classification\n",
    "4. Support Vector Machine (SVM)\n",
    "    - Quadratic Weighted Kappa (QWK)\n",
    "5. Kaggle Dataset \n",
    "6. Conclusion\n",
    "7. References\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-shade",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "alien-wales",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greater-duplicate",
   "metadata": {},
   "source": [
    "## 2. Importing, Description, and Modification of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "north-purchase",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('assignment2_data/Essay-Features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "becoming-italian",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1332 entries, 0 to 1331\n",
      "Data columns (total 19 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   essayid                    1332 non-null   int64  \n",
      " 1   chars                      1332 non-null   int64  \n",
      " 2   words                      1332 non-null   int64  \n",
      " 3   commas                     1332 non-null   int64  \n",
      " 4   apostrophes                1332 non-null   int64  \n",
      " 5   punctuations               1332 non-null   int64  \n",
      " 6   avg_word_length            1332 non-null   float64\n",
      " 7   sentences                  1332 non-null   int64  \n",
      " 8   questions                  1332 non-null   int64  \n",
      " 9   avg_word_sentence          1332 non-null   float64\n",
      " 10  POS                        1332 non-null   float64\n",
      " 11  POS/total_words            1332 non-null   float64\n",
      " 12  prompt_words               1332 non-null   int64  \n",
      " 13  prompt_words/total_words   1332 non-null   float64\n",
      " 14  synonym_words              1332 non-null   int64  \n",
      " 15  synonym_words/total_words  1332 non-null   float64\n",
      " 16  unstemmed                  1332 non-null   int64  \n",
      " 17  stemmed                    1332 non-null   int64  \n",
      " 18  score                      1332 non-null   int64  \n",
      "dtypes: float64(6), int64(13)\n",
      "memory usage: 197.8 KB\n"
     ]
    }
   ],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "practical-hobby",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essayid</th>\n",
       "      <th>chars</th>\n",
       "      <th>words</th>\n",
       "      <th>commas</th>\n",
       "      <th>apostrophes</th>\n",
       "      <th>punctuations</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>sentences</th>\n",
       "      <th>questions</th>\n",
       "      <th>avg_word_sentence</th>\n",
       "      <th>POS</th>\n",
       "      <th>POS/total_words</th>\n",
       "      <th>prompt_words</th>\n",
       "      <th>prompt_words/total_words</th>\n",
       "      <th>synonym_words</th>\n",
       "      <th>synonym_words/total_words</th>\n",
       "      <th>unstemmed</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1332.00000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.00000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.00000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>905.27027</td>\n",
       "      <td>2101.745495</td>\n",
       "      <td>424.485736</td>\n",
       "      <td>14.667417</td>\n",
       "      <td>8.141141</td>\n",
       "      <td>0.47973</td>\n",
       "      <td>4.939762</td>\n",
       "      <td>19.704204</td>\n",
       "      <td>1.222973</td>\n",
       "      <td>23.884687</td>\n",
       "      <td>420.596542</td>\n",
       "      <td>0.989935</td>\n",
       "      <td>198.913664</td>\n",
       "      <td>0.469164</td>\n",
       "      <td>110.16967</td>\n",
       "      <td>0.263846</td>\n",
       "      <td>468.987988</td>\n",
       "      <td>455.507508</td>\n",
       "      <td>3.427177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>526.68760</td>\n",
       "      <td>865.963750</td>\n",
       "      <td>171.873730</td>\n",
       "      <td>10.920781</td>\n",
       "      <td>6.124520</td>\n",
       "      <td>1.27168</td>\n",
       "      <td>0.231071</td>\n",
       "      <td>19.202731</td>\n",
       "      <td>1.847446</td>\n",
       "      <td>11.160020</td>\n",
       "      <td>170.985111</td>\n",
       "      <td>0.007308</td>\n",
       "      <td>82.729266</td>\n",
       "      <td>0.052466</td>\n",
       "      <td>43.96192</td>\n",
       "      <td>0.038870</td>\n",
       "      <td>159.447449</td>\n",
       "      <td>155.751220</td>\n",
       "      <td>0.774275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>169.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.231322</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.084112</td>\n",
       "      <td>35.647059</td>\n",
       "      <td>0.924771</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>11.00000</td>\n",
       "      <td>0.027299</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>442.75000</td>\n",
       "      <td>1527.250000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.791679</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.142857</td>\n",
       "      <td>305.406284</td>\n",
       "      <td>0.987758</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>0.435709</td>\n",
       "      <td>81.00000</td>\n",
       "      <td>0.238423</td>\n",
       "      <td>361.000000</td>\n",
       "      <td>350.750000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>914.50000</td>\n",
       "      <td>2029.500000</td>\n",
       "      <td>411.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.946059</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>22.030331</td>\n",
       "      <td>406.982869</td>\n",
       "      <td>0.991572</td>\n",
       "      <td>193.000000</td>\n",
       "      <td>0.465852</td>\n",
       "      <td>107.50000</td>\n",
       "      <td>0.262872</td>\n",
       "      <td>463.000000</td>\n",
       "      <td>448.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1369.75000</td>\n",
       "      <td>2613.500000</td>\n",
       "      <td>525.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>5.092938</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>26.048234</td>\n",
       "      <td>520.739458</td>\n",
       "      <td>0.994425</td>\n",
       "      <td>246.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>134.00000</td>\n",
       "      <td>0.288277</td>\n",
       "      <td>581.000000</td>\n",
       "      <td>561.250000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1799.00000</td>\n",
       "      <td>6142.000000</td>\n",
       "      <td>1170.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>26.00000</td>\n",
       "      <td>5.681429</td>\n",
       "      <td>642.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>1158.984563</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>669.000000</td>\n",
       "      <td>0.961207</td>\n",
       "      <td>355.00000</td>\n",
       "      <td>0.465517</td>\n",
       "      <td>750.000000</td>\n",
       "      <td>750.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          essayid        chars        words       commas  apostrophes  \\\n",
       "count  1332.00000  1332.000000  1332.000000  1332.000000  1332.000000   \n",
       "mean    905.27027  2101.745495   424.485736    14.667417     8.141141   \n",
       "std     526.68760   865.963750   171.873730    10.920781     6.124520   \n",
       "min       0.00000   169.000000    36.000000     0.000000     2.000000   \n",
       "25%     442.75000  1527.250000   310.000000     7.000000     4.000000   \n",
       "50%     914.50000  2029.500000   411.000000    13.000000     6.000000   \n",
       "75%    1369.75000  2613.500000   525.000000    21.000000    11.000000   \n",
       "max    1799.00000  6142.000000  1170.000000    72.000000    51.000000   \n",
       "\n",
       "       punctuations  avg_word_length    sentences    questions  \\\n",
       "count    1332.00000      1332.000000  1332.000000  1332.000000   \n",
       "mean        0.47973         4.939762    19.704204     1.222973   \n",
       "std         1.27168         0.231071    19.202731     1.847446   \n",
       "min         0.00000         2.231322     0.000000     0.000000   \n",
       "25%         0.00000         4.791679    13.000000     0.000000   \n",
       "50%         0.00000         4.946059    18.000000     1.000000   \n",
       "75%         0.00000         5.092938    24.000000     2.000000   \n",
       "max        26.00000         5.681429   642.000000    17.000000   \n",
       "\n",
       "       avg_word_sentence          POS  POS/total_words  prompt_words  \\\n",
       "count        1332.000000  1332.000000      1332.000000   1332.000000   \n",
       "mean           23.884687   420.596542         0.989935    198.913664   \n",
       "std            11.160020   170.985111         0.007308     82.729266   \n",
       "min             1.084112    35.647059         0.924771     14.000000   \n",
       "25%            19.142857   305.406284         0.987758    144.000000   \n",
       "50%            22.030331   406.982869         0.991572    193.000000   \n",
       "75%            26.048234   520.739458         0.994425    246.000000   \n",
       "max           303.000000  1158.984563         1.000000    669.000000   \n",
       "\n",
       "       prompt_words/total_words  synonym_words  synonym_words/total_words  \\\n",
       "count               1332.000000     1332.00000                1332.000000   \n",
       "mean                   0.469164      110.16967                   0.263846   \n",
       "std                    0.052466       43.96192                   0.038870   \n",
       "min                    0.288889       11.00000                   0.027299   \n",
       "25%                    0.435709       81.00000                   0.238423   \n",
       "50%                    0.465852      107.50000                   0.262872   \n",
       "75%                    0.500000      134.00000                   0.288277   \n",
       "max                    0.961207      355.00000                   0.465517   \n",
       "\n",
       "         unstemmed      stemmed        score  \n",
       "count  1332.000000  1332.000000  1332.000000  \n",
       "mean    468.987988   455.507508     3.427177  \n",
       "std     159.447449   155.751220     0.774275  \n",
       "min      48.000000    50.000000     1.000000  \n",
       "25%     361.000000   350.750000     3.000000  \n",
       "50%     463.000000   448.000000     3.000000  \n",
       "75%     581.000000   561.250000     4.000000  \n",
       "max     750.000000   750.000000     6.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepared-environment",
   "metadata": {},
   "source": [
    "### Modifications in df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "empty-diploma",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['stemmed/total_words'] = df1['stemmed']/df1['words']\n",
    "df1['unstemmed/total_words'] = df1['unstemmed']/df1['words']\n",
    "df1['related_words'] = df1['prompt_words'] + df1['POS'] + df1['synonym_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "given-algebra",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essayid</th>\n",
       "      <th>chars</th>\n",
       "      <th>words</th>\n",
       "      <th>commas</th>\n",
       "      <th>apostrophes</th>\n",
       "      <th>punctuations</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>sentences</th>\n",
       "      <th>questions</th>\n",
       "      <th>avg_word_sentence</th>\n",
       "      <th>...</th>\n",
       "      <th>prompt_words</th>\n",
       "      <th>prompt_words/total_words</th>\n",
       "      <th>synonym_words</th>\n",
       "      <th>synonym_words/total_words</th>\n",
       "      <th>unstemmed</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>score</th>\n",
       "      <th>stemmed/total_words</th>\n",
       "      <th>unstemmed/total_words</th>\n",
       "      <th>related_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>805</td>\n",
       "      <td>2422</td>\n",
       "      <td>495</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>4.892929</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>230</td>\n",
       "      <td>0.464646</td>\n",
       "      <td>130</td>\n",
       "      <td>0.262626</td>\n",
       "      <td>602</td>\n",
       "      <td>585</td>\n",
       "      <td>4</td>\n",
       "      <td>1.181818</td>\n",
       "      <td>1.216162</td>\n",
       "      <td>850.653117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1343</td>\n",
       "      <td>2581</td>\n",
       "      <td>510</td>\n",
       "      <td>22</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>5.060784</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>21.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>255</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>119</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>456</td>\n",
       "      <td>448</td>\n",
       "      <td>4</td>\n",
       "      <td>0.878431</td>\n",
       "      <td>0.894118</td>\n",
       "      <td>880.318898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>1597</td>\n",
       "      <td>2223</td>\n",
       "      <td>431</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5.157773</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>17.958333</td>\n",
       "      <td>...</td>\n",
       "      <td>221</td>\n",
       "      <td>0.512761</td>\n",
       "      <td>113</td>\n",
       "      <td>0.262181</td>\n",
       "      <td>480</td>\n",
       "      <td>462</td>\n",
       "      <td>4</td>\n",
       "      <td>1.071926</td>\n",
       "      <td>1.113689</td>\n",
       "      <td>762.990676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1140</th>\n",
       "      <td>231</td>\n",
       "      <td>1160</td>\n",
       "      <td>235</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4.936170</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>33.571429</td>\n",
       "      <td>...</td>\n",
       "      <td>100</td>\n",
       "      <td>0.425532</td>\n",
       "      <td>60</td>\n",
       "      <td>0.255319</td>\n",
       "      <td>294</td>\n",
       "      <td>290</td>\n",
       "      <td>3</td>\n",
       "      <td>1.234043</td>\n",
       "      <td>1.251064</td>\n",
       "      <td>392.991379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>440</td>\n",
       "      <td>4414</td>\n",
       "      <td>923</td>\n",
       "      <td>62</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>4.782232</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>23.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>349</td>\n",
       "      <td>0.378115</td>\n",
       "      <td>181</td>\n",
       "      <td>0.196100</td>\n",
       "      <td>750</td>\n",
       "      <td>750</td>\n",
       "      <td>5</td>\n",
       "      <td>0.812568</td>\n",
       "      <td>0.812568</td>\n",
       "      <td>1446.327517</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      essayid  chars  words  commas  apostrophes  punctuations  \\\n",
       "270       805   2422    495       9           13             0   \n",
       "767      1343   2581    510      22           20             0   \n",
       "1200     1597   2223    431       2            5             0   \n",
       "1140      231   1160    235       0            2             0   \n",
       "137       440   4414    923      62           51             0   \n",
       "\n",
       "      avg_word_length  sentences  questions  avg_word_sentence  ...  \\\n",
       "270          4.892929         15          6          33.000000  ...   \n",
       "767          5.060784         24          0          21.250000  ...   \n",
       "1200         5.157773         24          3          17.958333  ...   \n",
       "1140         4.936170          7          0          33.571429  ...   \n",
       "137          4.782232         39          1          23.666667  ...   \n",
       "\n",
       "      prompt_words  prompt_words/total_words  synonym_words  \\\n",
       "270            230                  0.464646            130   \n",
       "767            255                  0.500000            119   \n",
       "1200           221                  0.512761            113   \n",
       "1140           100                  0.425532             60   \n",
       "137            349                  0.378115            181   \n",
       "\n",
       "      synonym_words/total_words  unstemmed  stemmed  score  \\\n",
       "270                    0.262626        602      585      4   \n",
       "767                    0.233333        456      448      4   \n",
       "1200                   0.262181        480      462      4   \n",
       "1140                   0.255319        294      290      3   \n",
       "137                    0.196100        750      750      5   \n",
       "\n",
       "      stemmed/total_words  unstemmed/total_words  related_words  \n",
       "270              1.181818               1.216162     850.653117  \n",
       "767              0.878431               0.894118     880.318898  \n",
       "1200             1.071926               1.113689     762.990676  \n",
       "1140             1.234043               1.251064     392.991379  \n",
       "137              0.812568               0.812568    1446.327517  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-starter",
   "metadata": {},
   "source": [
    "### Features excluded from df1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "tamil-boring",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df1.drop(['essayid',\n",
    "              'score',\n",
    "              'punctuations',\n",
    "              'avg_word_length',\n",
    "              'avg_word_sentence',\n",
    "              'POS/total_words',\n",
    "              'prompt_words/total_words',\n",
    "              'synonym_words/total_words'\n",
    "             ],\n",
    "             axis = 1)\n",
    "y = df1['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "accessory-priority",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('assignment2_data/Essay-Features-Submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "traditional-patrol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 199 entries, 0 to 198\n",
      "Data columns (total 18 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   essayid                    199 non-null    int64  \n",
      " 1   chars                      199 non-null    int64  \n",
      " 2   words                      199 non-null    int64  \n",
      " 3   commas                     199 non-null    int64  \n",
      " 4   apostrophes                199 non-null    int64  \n",
      " 5   punctuations               199 non-null    int64  \n",
      " 6   avg_word_length            199 non-null    float64\n",
      " 7   sentences                  199 non-null    int64  \n",
      " 8   questions                  199 non-null    int64  \n",
      " 9   avg_word_sentence          199 non-null    float64\n",
      " 10  POS                        199 non-null    float64\n",
      " 11  POS/total_words            199 non-null    float64\n",
      " 12  prompt_words               199 non-null    int64  \n",
      " 13  prompt_words/total_words   199 non-null    float64\n",
      " 14  synonym_words              199 non-null    int64  \n",
      " 15  synonym_words/total_words  199 non-null    float64\n",
      " 16  unstemmed                  199 non-null    int64  \n",
      " 17  stemmed                    199 non-null    int64  \n",
      "dtypes: float64(6), int64(12)\n",
      "memory usage: 28.1 KB\n"
     ]
    }
   ],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-march",
   "metadata": {},
   "source": [
    "### Modifications in df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "heard-fifty",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2['stemmed/total_words'] = df2['stemmed']/df2['words']\n",
    "df2['unstemmed/total_words'] = df2['unstemmed']/df2['words']\n",
    "df2['related_words'] = df2['prompt_words'] + df2['POS'] + df2['synonym_words']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ethical-quarterly",
   "metadata": {},
   "source": [
    "## 3. Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoking-asbestos",
   "metadata": {},
   "source": [
    "Supervised machine learning also known as supervised learning can be intepreted by the manipulation of labeled datasets to train algorithms that are able to categorise data or accurately predict results. In this df1 dataset, x is the input data and y is the labeled data. Data labeling is an important in data preprocessing for supervised machine learning, where both input and output data are labeled for classification to provide a learning basis for future data processing.\n",
    "\n",
    "The implementation of training set on test set is crucial to validate the model built. The models generated are to predict the unknown score which is named as the test set. The sklearn.model_selection.train_test_split() is used to randomly split arrays or matrices into training and testing subsets, which are x_train, x_test, y_train, y_test. In this case, value of 0.25 is set as the test size, thus the 25% of dataset is splited as the test dataset and the remaining 75% are training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "scenic-scale",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    " x, y, test_size = 0.25, random_state = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-lyric",
   "metadata": {},
   "source": [
    "### Normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "administrative-korea",
   "metadata": {},
   "source": [
    "When the distribution of data is unknown or the data doesn't not normally distributed, normalising data is important to leverage the numerical data without distorting the differences in the ranges of values. For instance, consider this df1 dataset containing two features, punctuations, and characters. Where punctuation ranges from 0 to 1, whereas character ranges from 236 to 4332. Thus, data normalisation allows to the variable to be at the same range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "respective-knitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train) \n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experimental-princess",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-weekly",
   "metadata": {},
   "source": [
    "**Classification** is a supervised learning technique that categorize an unseen observation into one of the sub-groups that exist in the **training dataset**. It is supervised learning because the training data has prior knowledge of the categories as it has the target labels (**labeled dataset**). \n",
    "\n",
    "Difference between binary and multi-class classification: \n",
    "\n",
    "- **Binary classification**\n",
    "\n",
    "   Binary classification are tasks where specimens are assigned only one of two classes (Brownlee 2021). Examples include logistic Regression, Support Vector Machines (SVM), and more are algorithms designed for binary classification problems.\n",
    "   \n",
    "\n",
    "- **multi-class classification**\n",
    "\n",
    "    In contrary to the multi-class classification which specimens are assigned only one of more than two classes. Heuristic approaches for multi-class classification tasks including **One-vs-Rest (OvR)** and **One-vs-One (OvO)** split a multi-class classification problem into multiple classification datasets and train each binary classification model (Brownlee 2021)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-introduction",
   "metadata": {},
   "source": [
    "## 4. Support Vector Machine "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-bandwidth",
   "metadata": {},
   "source": [
    "Support vector machines (SVM) is a linear model consists a set of supervised learning techniques used for classification, regression, and outliers identification. There are specialised SVMs such as **support vector regression (SVR)** and **support vector classification (SVC)** designed to tackle particular machine learning problems (Pupale, 2018). There are two different types of SVMs, one is the simple SVM generally applied for linear regression and classification problems . \n",
    "\n",
    "Besides, SVMs also use **Kernel** functions to systematically find support vector classifiers in higher dimensions as it provides more flexibility for non-linear data so that more features can be added to fit a hyperplane instead of a two-dimensional space. The functions include **linear**, **polynomial**, **Gaussian Radial Basis Function (RBF)**, **Sigmoid**, and others (Awasthi 2020). \n",
    "\n",
    "RBF kernel SVM is applied in this report in order to target non-linear problems. Although RBF is a non-linear kernel, it utilises linear dicision region to generate non-linear combinations of the dataset's features to elevate the sample data onto a higher dimension space where linear hypersurface that partitions the underlying vector space into two sets, one for each class.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "respected-regular",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel = 'rbf',\n",
    "              gamma = 'auto')\n",
    "clf.fit(X_train,y_train)\n",
    "pred_clf = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-plumbing",
   "metadata": {},
   "source": [
    "### Confusion matrix  (Wisdom 2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-lesbian",
   "metadata": {},
   "source": [
    "This is a 6-class matrix with essay scores ranging from 1 to 6. \n",
    "\n",
    "Total number of true positives (classes in diagonal):\n",
    "- score 1: 1\n",
    "- score 2: 13\n",
    "- score 3: 96\n",
    "- score 4: 124\n",
    "- score 5: 1\n",
    "- score 6: 0\n",
    "                               \n",
    "Total number of false negatives (classes in corresponding rows):\n",
    "- score 1: 3 + 1 = 4\n",
    "- score 2: 10 + 1 = 11\n",
    "- score 3: 2 + 35 = 37\n",
    "- score 4: 32\n",
    "- score 5: 13\n",
    "- score 6: 1\n",
    "                                \n",
    "Total number of false positives (classes in corresponding columns):\n",
    "- score 1: 0\n",
    "- score 2: 3 + 2 = 5\n",
    "- score 3: 1 + 10 + 32 = 43\n",
    "- score 4: 1 + 35 + 13 + 1 = 50\n",
    "- score 5: 1\n",
    "- score 6: 0\n",
    "\n",
    "Total number of true negatives (sum of all columns and rows of certain class excluding others class's column and row):\n",
    "- score 1: 13 + 10 + 1 + 2 + 96 + 35 + 32 + 124 + 13 + 1 + 1 = 328\n",
    "- score 2: 1 + 1 + 96 + 35 + 32 + 124 + 13 + 1 + 1 = 304\n",
    "- score 3: 1 + 3 + 13 + 1 + 124 + 13 + 1 + 1 = 157\n",
    "- score 4: 1 + 3 + 1 + 13 + 10 + 2 + 96 + 1 = 127\n",
    "- score 5: 1 + 3 + 1 + 13 + 10 + 1 + 2 + 96 + 35 + 32 + 124 + 1 = 319\n",
    "- score 6: 1 + 3 + 1 + 13 + 10 + 1 + 2 + 96 + 35 + 32 + 124 + 13 + 1 = 332"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "interior-solid",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1   3   1   0   0   0]\n",
      " [  0  13  10   1   0   0]\n",
      " [  0   2  96  35   0   0]\n",
      " [  0   0  32 124   0   0]\n",
      " [  0   0   0  13   1   0]\n",
      " [  0   0   0   1   0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.20      0.33         5\n",
      "           2       0.72      0.54      0.62        24\n",
      "           3       0.69      0.72      0.71       133\n",
      "           4       0.71      0.79      0.75       156\n",
      "           5       1.00      0.07      0.13        14\n",
      "           6       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.71       333\n",
      "   macro avg       0.69      0.39      0.42       333\n",
      "weighted avg       0.72      0.71      0.69       333\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,pred_clf))\n",
    "print(classification_report(y_test,pred_clf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extended-transaction",
   "metadata": {},
   "source": [
    "Precision of each score are above 0.7 due to the low to no false positives found within the class. There are two classes (score 3 and 4) has sensitivity (recall) greater than 0.7. Score 5 shows the lowest recall of 0.07 because it has high number of false negative to true positive ratio (13 : 1). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resistant-astronomy",
   "metadata": {},
   "source": [
    "### Quadratic Weighted Kappa (QWK)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-wiring",
   "metadata": {},
   "source": [
    "Quadratic weighted kappa (QWK) measures the agreement between two ratings. QWk typically changes from 0 to 1, which measures the agreement between raters from random to complete (Arora 2019). When the metric go below 0, there is less agreement between the raters than expected by chance. The QWK is calculated between the scores which are expected and the predicted scores. \n",
    "\n",
    "A perfect QWK score of 1.0 is granted when both the predictions and actuals are the same. Whereas, the least possible score is -1 which is given when the predictions are furthest away from actuals (Arora 2019). The aim is to get as close to 1 as possible. This model shows a general QWK score of 0.667 which indicates high aggreement between the predicted and test score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abroad-snowboard",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.667338879085791"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohen_kappa_score(y_test, pred_clf, labels=None, weights= 'quadratic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rising-system",
   "metadata": {},
   "source": [
    "## 5. Kaggle Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "divided-denial",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = df2.drop(['essayid',\n",
    "                   'punctuations',\n",
    "                   'avg_word_length',\n",
    "                   'avg_word_sentence',\n",
    "                   'POS/total_words',\n",
    "                   'prompt_words/total_words',\n",
    "                   'synonym_words/total_words'\n",
    "                  ],\n",
    "                  axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "agricultural-crisis",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df2 = sc.fit_transform(x_test)\n",
    "y_pred_df2 = clf.predict(x_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "behavioral-waterproof",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv('assignment2_data/test1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "joined-intervention",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kaggle = df3.assign(score = y_pred_df2)\n",
    "df_kaggle.to_csv(\"assignment2_data/test2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "documentary-bloom",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essayid</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1623</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1143</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>660</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1596</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>846</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>868</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>145</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>500</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>652</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>214</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essayid  score\n",
       "0     1623      4\n",
       "1     1143      3\n",
       "2      660      3\n",
       "3     1596      4\n",
       "4      846      4\n",
       "5      868      4\n",
       "6      145      3\n",
       "7      500      3\n",
       "8      652      3\n",
       "9      214      3"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kaggle.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-boost",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "christian-kansas",
   "metadata": {},
   "source": [
    "As a summary, the trained model was tested with the generated test dataset. The outcome of result showed a standard QWK score of 0.656, with an accuracy of up to 71% on the test dataset. There are only several features selected for the training model, however some features has been excluded including 'essayid', 'punctuations', 'avg_word_length', 'avg_word_sentence', 'POS/total_words', 'prompt_words/total_words', and 'synonym_words/total_words' which are appeared to be less significant in predicting the score. Although the dataset provided is not sufficient in predicting score with high level of accuracy and precision, as type of words and vocabulary used are also important in determining the essay score. However, overall the developed model is dependable and can be upgraded by applying more features and training on different type of models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-helping",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-fifteen",
   "metadata": {},
   "source": [
    "Arora, A., 2019. Quadratic Kappa Metric explained in 5 simple steps. [online] Kaggle.com. Available at: <https://www.kaggle.com/aroraaman/quadratic-kappa-metric-explained-in-5-simple-steps> [Accessed 27 April 2021].\n",
    "\n",
    "Awasthi, S., 2020. Seven Most Popular SVM Kernels. [online] Dataaspirant. Available at: <https://dataaspirant.com/svm-kernels/> [Accessed 27 April 2021].\n",
    "\n",
    "Brownlee, J., 2021. One-vs-Rest and One-vs-One for Multi-Class Classification. [online] Machine Learning Mastery. Available at: <https://machinelearningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/#:~:text=Binary%20classification%20are%20those%20tasks,Classification%20tasks%20with%20two%20classes.> [Accessed 27 April 2021].\n",
    "\n",
    "Pupale, R., 2018. Support Vector Machines(SVM) — An Overview. [online] Towards Data Science. Available at: <https://towardsdatascience.com/https-medium-com-pupalerushikesh-svm-f4b42800e989#:~:text=SVM%20or%20Support%20Vector%20Machine,separates%20the%20data%20into%20classes.> [Accessed 27 April 2021].\n",
    "\n",
    "Wisdom, B., 2019. Understanding the Confusion Matrix (II). [online] DEV Community. Available at: <https://dev.to/overrideveloper/understanding-the-confusion-matrix-264i#:~:text=%E2%80%8B%20Precision%20is%20a%20multi,True%20Positives%20and%20False%20Positives.> [Accessed 27 April 2021]."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
